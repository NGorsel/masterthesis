{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets read the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pizza hut favorite restaurant'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RemoveStops(three, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "one = 'hello i love pizza'\n",
    "two = 'where is my pizza delivery guy'\n",
    "three = 'pizza hut is my favorite restaurant'\n",
    "\n",
    "\n",
    "    \n",
    "for sentence in [one,two,three]:\n",
    "    cleaned_sentence = RemoveStops(sentence, 'english')    \n",
    "    for word in cleaned_sentence.split(' '):\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery</th>\n",
       "      <th>favorite</th>\n",
       "      <th>guy</th>\n",
       "      <th>hello</th>\n",
       "      <th>hut</th>\n",
       "      <th>love</th>\n",
       "      <th>pizza</th>\n",
       "      <th>restaurant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_two</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_three</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery  favorite  guy  hello  hut  love  pizza  restaurant\n",
       "doc_one           0         0    0      1    0     1      1           0\n",
       "doc_two           1         0    1      0    0     0      1           0\n",
       "doc_three         0         1    0      0    1     0      1           1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({ 'doc_one' : {'delivery' : 0, 'favorite' : 0, 'guy' : 0, 'hello' : 1, 'hut' : 0, 'love': 1, 'pizza' : 1, 'restaurant' : 0},\n",
    " 'doc_two' : {'delivery' : 1, 'favorite' : 0, 'guy' : 1, 'hello' : 0, 'hut' : 0, 'love': 0, 'pizza' : 1 , 'restaurant' : 0},\n",
    " 'doc_three' : {'delivery' : 0, 'favorite' : 1, 'guy' : 0, 'hello' : 0, 'hut' : 1, 'love' : 0, 'pizza': 1, 'restaurant' : 1} }).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voetbaldirect_followerProfile.csv',\n",
       " 'voetbaldirect_second_followerProfile.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def folderCompProfiles(comp_name):\n",
    "    \"\"\"Searches in the data folder to search for followerProfiles on the name of this company.\"\"\"\n",
    "    #Get all the followerProfiles\n",
    "    followerProfiles = [value for value in os.listdir(\"insta_followerprofile/data\") if 'followerProfile' in value]\n",
    "    company_followerProfiles = [value for value in followerProfiles if comp_name + '_' in value]\n",
    "    return company_followerProfiles\n",
    "\n",
    "folderCompProfiles('voetbaldirect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeRawDataFrame(file_locations):\n",
    "    \"\"\"Takes as input a list of file locations. First checks the usernames present in all files and notifies\n",
    "    the user if there is an overlap. After this it merges all the follower profiles.\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    name_buffer = {}\n",
    "    for followerProfile in file_locations:\n",
    "        temp_df = pd.read_csv(\"../data/rawdata/\" + followerProfile)\n",
    "        names = [value for value in set([value for value in temp_df['data.user.edge_owner_to_timeline_media.edges.node.owner.username']])]\n",
    "        #Small check to see whether names are present in different followerProfiles of the same company.\n",
    "        for name in names:\n",
    "            if name in name_buffer.keys():\n",
    "                print('We have a double name: {}'.format(name))\n",
    "                name_buffer[name] += 1\n",
    "            else:\n",
    "                name_buffer[name] = 1\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PLEASE READ: ONLY READ THIS WHEN USING THE NEW WORKFLOW OF STEVESIE DATA. OTHERWISE THE TEMPLATE IS DIFFERENT!\n",
    "\n",
    "def cleanRawDataFrame(raw_dataframe):\n",
    "    \"\"\"Select the desired columns from the raw dataframe and renames the columns.\"\"\"\n",
    "    cleaned_df = raw_dataframe[[\n",
    "         'input.user_id', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.owner.username', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.shortcode', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.edge_media_to_caption.edges[0].node.text', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.display_url', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.video_url', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.edge_media_preview_like.count', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.edge_media_to_comment.count', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.video_view_count', \n",
    "         'data.user.edge_owner_to_timeline_media.edges.node.taken_at_timestamp']]\n",
    "    \n",
    "    new_columnNames = ['userID',\n",
    "                      'username',\n",
    "                      'shortcode',\n",
    "                      'original_text',\n",
    "                      'display_url',\n",
    "                      'video_url',\n",
    "                      'like_count',\n",
    "                      'comment_count',\n",
    "                       'view_count',\n",
    "                      'timestamp']\n",
    "    \n",
    "    cleaned_df.columns = new_columnNames\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create seperate column which indicate occurance of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def cleanUnicode(string):\n",
    "    \"\"\"Some simple cleanup applied to a string of text\"\"\"\n",
    "    string = string.lower() #No capital letters\n",
    "    string = string.replace('\\n',' ') # Remove new line commands\n",
    "    string = string.replace('\\xa0',' ') # Remove non-breaking space in Latin\n",
    "    return string\n",
    "    \n",
    "    \n",
    "def removeHashtags(string):\n",
    "    \"\"\"Takes string as input. Removes all words which start with # with length > 2\"\"\"\n",
    "    sentence_words = string.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence_words:\n",
    "        if word[0] == '#' and len(word) > 2:\n",
    "            continue\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    return ' '.join(new_sentence)\n",
    "            \n",
    "    \n",
    "def extractHashtags(string):\n",
    "    '''This function simply takes a string as input and analyses all words that start with a hashtag.'''\n",
    "    buffer = []\n",
    "    try:\n",
    "        for value in string.split(' '):\n",
    "            try:\n",
    "                if value[0] == '#':\n",
    "                    #Counter the number of hashtags in the hashtag. If > 0 we need to further split.\n",
    "                    if dict(Counter(value))['#'] > 1:\n",
    "                        multiple_hashtags = value.split('#')\n",
    "                        for hashtag in multiple_hashtags:\n",
    "                            if len(hashtag) > 2:\n",
    "                                buffer.append('#' + hashtag)\n",
    "                    #Only one hashtag in the word? Just add it to the buffer if length > 2.\n",
    "                    else:\n",
    "                        if len(value) > 2: buffer.append(value)\n",
    "            except IndexError:\n",
    "                continue\n",
    "    except SyntaxError:\n",
    "        print('oei')\n",
    "    return buffer\n",
    "\n",
    "def extractUser(string):\n",
    "    '''This function simply takes a string as input and analyses all words that start with a hashtag.'''\n",
    "    buffer = []\n",
    "    try:\n",
    "        for value in string.split(' '):\n",
    "            try:\n",
    "                if value[0] == '@':\n",
    "                    #Counter the number of hashtags in the hashtag. If > 0 we need to further split.\n",
    "                    if dict(Counter(value))['@'] > 1:\n",
    "                        multiple_hashtags = value.split('@')\n",
    "                        for hashtag in multiple_hashtags:\n",
    "                            if len(hashtag) > 2:\n",
    "                                buffer.append('@' + hashtag)\n",
    "                    #Only one hashtag in the word? Just add it to the buffer if length > 2.\n",
    "                    else:\n",
    "                        if len(value) > 2: buffer.append(value)\n",
    "            except IndexError:\n",
    "                continue\n",
    "    except SyntaxError:\n",
    "        print('oei')\n",
    "    return buffer\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return emoji.get_emoji_regexp().sub(u'', inputString)\n",
    "\n",
    "def removePunct_ONEWORD(string):\n",
    "    \"\"\"This function uses a string of words as input and cleans it as good as it can.\"\"\"\n",
    "    string= string.replace('.', ' ')\n",
    "    temp = re.sub(r'[^\\w\\s]','',string)\n",
    "    return temp.replace(' ', '.')\n",
    "\n",
    "def RemoveStops(string, lang):\n",
    "    \"\"\"This takes a string as input, splits it on the spaces and removes the stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words(lang)) \n",
    "    temp = [word for word in string.split(\" \") if not word in stop_words]\n",
    "    return ' '.join(temp)\n",
    "\n",
    "def processText(original_string):\n",
    "    #Remove capital and some weird unicode.\n",
    "    string = cleanUnicode(original_string)\n",
    "    #Extract user mentions and hashtags so we can remove this from the text.\n",
    "    hashtags = extractHashtags(string)\n",
    "    userMentions = extractUser(string)\n",
    "    #Removing the hashtags and user mentions seperately from the list of words.\n",
    "    string_noHashtags = [value for value in word_tokenize(string) if '#' + value not in hashtags]\n",
    "    string_noUsers = [value for value in string_noHashtags if value[1::] not in userMentions]\n",
    "    #Add all words together into a sentence if the word length > 1. (remove punc etc.)\n",
    "    processed_text =  ' '.join([value for value in string_noUsers if len(value) > 1])\n",
    "    #Finally remove the emoticons and we are done!\n",
    "    hashtags_cleaned = [removePunct_ONEWORD(value) for value in hashtags]\n",
    "    userMentions_cleaned = [removePunct_ONEWORD(value) for value in userMentions]\n",
    "    #Now lets remove some dutch and English stopwords from the processed text.\n",
    "    processed_text_noStops = RemoveStops(RemoveStops(processed_text, 'dutch'), 'english')\n",
    "    #Finally remove the emoticons from the no stopwords text and we are done!\n",
    "    processed_text_noEmoticons = deEmojify(processed_text_noStops)\n",
    "    return [original_string, processed_text_noEmoticons, hashtags_cleaned, userMentions_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProcessedSenteceDF(followerProfile_sentences):\n",
    "    \"\"\"Takes the written texts of the posts from a followerProfile as input. Extract hashtags and usernames to\n",
    "    create three seperate columns in the dataframe to increase processing.\"\"\"\n",
    "    df_buffer = []\n",
    "    for sentence in sentences:   \n",
    "        #If the text is just NaN (float) we want to skip it.\n",
    "        if isinstance(sentence, float) == True:\n",
    "            df_buffer.append(['', '', '', ''])\n",
    "        else:\n",
    "            df_buffer.append(processText(sentence))\n",
    "\n",
    "    return pd.DataFrame(df_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = [\"madedotcom\", \"vtwonen\", \"hemanederland\", \"loods5\", \"ikeanederland\", \"homify\", \"westwingnl\", \n",
    "\"karwei\", \"kwantum_nederland\", \"xenos_nl\", \"homedeco\", \"bol_com\", \"leenbakker\", \"wonenmetlef\", \"_connox_\",\n",
    "\"interiorjunkiecom\", \"jysknl\", \"wehkamp\", \"fonqnl\", \"konforhome\", \"basiclabel.nl\", \"blokker\",\n",
    "\"deensnl\", \"hastensbeds\", \"eijerkamp\", \"goossenswonenenslapen\", \"furn.nl\", \"stoermetaal\", \"roomednl\", \"misterdesignnl\",\n",
    "\"dekbeddiscounter\", \"woonexpress\", \"zitmaxx\", \"pronto_wonen\", \"designbestseller\", \"barbecueshop.nl\",\n",
    "\"flinders.design\", \"trendhopper\", \"debommelmeubelen\", \"otto_nl\", \"praxis_bouwmarkt\", \"gamma_nl\",\n",
    "\"pietklerkx.nl\", \"swisssense\", \"montelwonen\", \"aupingnl\", \"hacowonenenslapen\", \"emma_matras\", \"hornbachnl\",\n",
    "\"lampenlicht.nl\", \"profijtmeubel\", \"bianonl\", \"woonboulevardpoortvliet\", \"morreswonen\", \"hubo_nl\", \"beter_bed\",\n",
    "\"hoogenboezem.meubelen\", \"villajipp_outlet\", \"vidaxl_nl\", \"mline_nl\"]\n",
    "\n",
    "food_companies = [\"veganjunkfoodbar\", \"pastaebasta_amsterdam\", \"mamakellyamsterdam\", \"watsonsfood\",\n",
    "\"cannibaleroyale\", \"parkheuvel\", \"restaurantfred\", \"hugh_rotterdam\", \"oldscuola\", \"restaurantkite\",\n",
    "\"wturbankitchen\", \"thestreetfoodclub\", \"rumclubutrecht\", \"lejardinutrecht\", \"broei.utrecht\"]\n",
    "\n",
    "sport_companies = [\"plutosport.nl\", \"voetbalshopnl\", \"all4runningstore\", \"voetbaldirect\",\n",
    "\"dakasport\", \"hockeydirect.nl\", \"tennisdirect\", \"intersportnl\", \"aktiesport.nl\", \"sport2000nederland\",\n",
    "\"soccerfanshop\", \"jdsportsnl\", \"decathlonnederland\", \"gorillasportsnl\", \"perrysport.nl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c29f81a2966e>:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning profile of perrysport.nl. Total IDS: 2003.\n"
     ]
    }
   ],
   "source": [
    "for comp_name in sport_companies:    \n",
    "    #Find all company follower profiles in the data folder\n",
    "    company_followerProfiles = folderCompProfiles(comp_name)\n",
    "    #Merge all company follower profiles in one final dataframe\n",
    "    cleaned_df = mergeRawDataFrame(company_followerProfiles)\n",
    "    #Give the columns of the new dataframe the desired column names.\n",
    "    cleaned_df = cleanRawDataFrame(cleaned_df)\n",
    "    #Storing all the texts of all posts under one variable name\n",
    "    sentences = [value for value in cleaned_df['original_text']]\n",
    "    #Creating a new dataframe which contains all the processed posts\n",
    "    cleaned_sentence_df = createProcessedSenteceDF(sentences)\n",
    "    #Add these columns of the cleaned sentence dataframe to the cleaned dataframe we already have\n",
    "    cleaned_df['cleaned_text'] = [value for value in cleaned_sentence_df[1]]\n",
    "    cleaned_df['hashtags'] = [value for value in cleaned_sentence_df[2]]\n",
    "    cleaned_df['userMsentions'] = [value for value in cleaned_sentence_df[3]]\n",
    "    #Printing the total number of userIDs from which we extracted the posts now.\n",
    "    total_userIDS = len(set([value for value in cleaned_df['userID']]))\n",
    "    #Finally storing the final dataframe in the data folder.\\\n",
    "    print(\"Finished cleaning profile of {}. Total IDS: {}.\".format(comp_name, total_userIDS))\n",
    "    cleaned_df.to_pickle(\"../data/cleanedprofiles/{}_cleaned.pkl\".format(comp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "<ipython-input-5-c29f81a2966e>:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plutosport.nl - 697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a double name: mishaa_79_\n",
      "voetbalshopnl - 1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0,1,3,6,8,11,12,13,14,20,22,24,26,31,39,40,41,42,43,46,47,75,76,89,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all4runningstore - 2046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voetbaldirect - 688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0,1,3,11,13,14,16,19,20,22,23,24,25,27,29,30,31,32,33,34,47,89,90,91,92,93,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dakasport - 1321\n",
      "hockeydirect.nl - 1549\n",
      "tennisdirect - 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (87,88,89,90,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (88,89,90,91,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersportnl - 2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (88,89,90,91,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aktiesport.nl - 2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport2000nederland - 937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0,2,6,7,8,12,13,14,16,18,19,21,26,28,29,30,32,33,44,45,88,89,90,91,93,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccerfanshop - 1913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/nielsvangorsel/.pyenv/versions/3.8.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0,7,11,12,13,14,16,18,19,21,29,30,31,32,33,50,88,89,90,91,93,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdsportsnl - 1980\n",
      "decathlonnederland - 2719\n",
      "gorillasportsnl - 1059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ea8aeb075ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msport_companies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcheck_IDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-ea8aeb075ccc>\u001b[0m in \u001b[0;36mcheck_IDS\u001b[0;34m(comp_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcompany_followerProfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolderCompProfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#Merge all company follower profiles in one final dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcleaned_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeRawDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_followerProfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Give the columns of the new dataframe the desired column names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcleaned_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanRawDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c29f81a2966e>\u001b[0m in \u001b[0;36mmergeRawDataFrame\u001b[0;34m(file_locations)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfollowerProfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_locations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insta_followerprofile/data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfollowerProfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data.user.edge_owner_to_timeline_media.edges.node.owner.username'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#Small check to see whether names are present in different followerProfiles of the same company.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def check_IDS(comp_name):\n",
    "    #Find all company follower profiles in the data folder\n",
    "    company_followerProfiles = folderCompProfiles(comp_name)\n",
    "    #Merge all company follower profiles in one final dataframe\n",
    "    cleaned_df = mergeRawDataFrame(company_followerProfiles)\n",
    "    #Give the columns of the new dataframe the desired column names.\n",
    "    cleaned_df = cleanRawDataFrame(cleaned_df)\n",
    "    total_userIDS = len(set([value for value in cleaned_df['userID']]))\n",
    "    print(\"{} - {}\".format(comp_name, total_userIDS))\n",
    "    return \n",
    "\n",
    "for comp in sport_companies:\n",
    "    check_IDS(comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
