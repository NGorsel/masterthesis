{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_companies = [\"veganjunkfoodbar\", \"pastaebasta_amsterdam\", \"mamakellyamsterdam\", \"watsonsfood\",\n",
    "\"cannibaleroyale\", \"parkheuvel\", \"restaurantfred\", \"hugh_rotterdam\", \"oldscuola\", \"restaurantkite\",\n",
    "\"wturbankitchen\", \"thestreetfoodclub\", \"rumclubutrecht\", \"lejardinutrecht\", \"broei.utrecht\"]\n",
    "\n",
    "furniture_companies = [\"madedotcom\", \"vtwonen\", \"hemanederland\", \"loods5\", \"ikeanederland\", \"homify\", \"westwingnl\", \n",
    "\"karwei\", \"kwantum_nederland\", \"xenos_nl\", \"homedeco\", \"bol_com\", \"leenbakker\", \"wonenmetlef\", \"_connox_\",\n",
    "\"interiorjunkiecom\", \"jysknl\", \"wehkamp\", \"fonqnl\", \"konforhome\", \"basiclabel.nl\", \"blokker\",\n",
    "\"deensnl\", \"hastensbeds\", \"eijerkamp\", \"goossenswonenenslapen\", \"furn.nl\", \"stoermetaal\", \"roomednl\", \"misterdesignnl\",\n",
    "\"dekbeddiscounter\", \"woonexpress\", \"zitmaxx\", \"pronto_wonen\", \"designbestseller\", \"barbecueshop.nl\",\n",
    "\"flinders.design\", \"trendhopper\", \"debommelmeubelen\", \"otto_nl\", \"praxis_bouwmarkt\", \"gamma_nl\",\n",
    "\"pietklerkx.nl\", \"swisssense\", \"montelwonen\", \"aupingnl\", \"hacowonenenslapen\", \"emma_matras\", \"hornbachnl\",\n",
    "\"lampenlicht.nl\", \"profijtmeubel\", \"bianonl\", \"woonboulevardpoortvliet\", \"morreswonen\", \"hubo_nl\", \"beter_bed\",\n",
    "\"hoogenboezem.meubelen\", \"villajipp_outlet\", \"vidaxl_nl\", \"mline_nl\"]\n",
    "\n",
    "sport_companies = [\"plutosport.nl\", \"voetbalshopnl\", \"all4runningstore\", \"voetbaldirect\",\n",
    "\"dakasport\", \"hockeydirect.nl\", \"tennisdirect\", \"intersportnl\", \"aktiesport.nl\", \"sport2000nederland\",\n",
    "\"soccerfanshop\", \"jdsportsnl\", \"decathlonnederland\", \"gorillasportsnl\", \"perrysport.nl\"]\n",
    "\n",
    "company_names = food_companies + furniture_companies + sport_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Create/load the ground truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 Interview ground standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_interviews = {\n",
    "            'aupingnl' : {'ikeanederland' : 3,\n",
    "                       'swisssense': 3,\n",
    "                       'hastensbeds' : 2,\n",
    "                       'beter_bed' : 2,\n",
    "                       'mline_nl' : 2,\n",
    "                       'emma_matras' : 1\n",
    "                        },\n",
    "            'fonqnl' : {'ikeanederland' : 3,\n",
    "                       'bol_com' : 3,\n",
    "                       'wehkamp' : 3,\n",
    "                       'leenbakker' : 2,\n",
    "                       'madedotcom' : 2,\n",
    "                       'kwantum_nederland' : 2,\n",
    "                       'blokker' : 1,\n",
    "                       'hemanederland' : 1,\n",
    "                       'xenos_nl' : 1,\n",
    "                       'westwingnl' : 1,\n",
    "                       'flinders.design' : 1\n",
    "                       },\n",
    "            'swisssense' : {'aupingnl' : 3,\n",
    "                       'beter_bed' : 3,\n",
    "                       'goossenswonenenslapen' : 3,\n",
    "                       'ikeanederland' : 2,\n",
    "                       'leenbakker' : 2,\n",
    "                       'jysknl' : 2,\n",
    "                       'kwantum_nederland' : 1\n",
    "                        },\n",
    "     'flinders.design' : {'misterdesignnl' : 3,\n",
    "                          'fonqnl' : 4,\n",
    "                       '_connox_' : 3,\n",
    "                        'deensnl' : 3,\n",
    "                       'designbestseller' : 2,\n",
    "                        'trendhopper' : 2,\n",
    "                        'interiorjunkiecom' : 2,\n",
    "                       'ikeanederland' : 2\n",
    "                        },\n",
    "        'leenbakker' : {'ikeanederland' : 2,\n",
    "                       'kwantum_nederland' : 2,\n",
    "                       'fonqnl' : 1,\n",
    "                       'jysknl' : 2,\n",
    "                       'wehkamp' : 1,\n",
    "                       'beter_bed' : 1,\n",
    "                       'bol_com' : 1,\n",
    "                       'dekbeddiscounter' : 1\n",
    "                       },\n",
    "            'karwei' : {'gamma_nl' : 5,\n",
    "                       'praxis_bouwmarkt' : 4,\n",
    "                       'ikeanederland' : 4,\n",
    "                       'hornbachnl' : 3,\n",
    "                       'kwantum_nederland' : 3,\n",
    "                       'leenbakker' : 3,\n",
    "                        'jysknl': 3,\n",
    "                        'hubo_nl' : 2,\n",
    "                        'loods5': 1\n",
    "                       },\n",
    "            'loods5' : {'ikeanederland' : 1,\n",
    "                       'kwantum_nederland' : 1,\n",
    "                       'leenbakker' : 1,\n",
    "                       'bol_com' : 1,\n",
    "                       'wehkamp' : 1,\n",
    "                       'fonqnl' : 1,\n",
    "                       'madedotcom' : 1,\n",
    "                       },\n",
    "    'debommelmeubelen' : {'hacowonenenslapen' : 2,\n",
    "                       'goossenswonenenslapen' : 3,\n",
    "                       'zitmaxx' : 3,\n",
    "                        'eijerkamp' : 2,\n",
    "                        'flinders.design' : 2,\n",
    "                        'pietklerkx.nl' : 2\n",
    "                        },\n",
    "'goossenswonenenslapen' : {'fonqnl' : 3,\n",
    "                       'pietklerkx.nl' : 2,\n",
    "                       'eijerkamp' : 2,\n",
    "                       'vtwonen' : 1,\n",
    "                       'madedotcom' : 1,\n",
    "                       'loods5' : 1,\n",
    "                       'ikeanederland' : 1,\n",
    "                       'swisssense' : 1,\n",
    "                       'bol_com' : 1,\n",
    "                       'konforhome' : 1,\n",
    "                       'zitmaxx' : 1,\n",
    "                       'flinders.design' : 1\n",
    "                        }}\n",
    "\n",
    "#Transform answers to a list\n",
    "gt_interviews_list = {}\n",
    "\n",
    "for company in gt_interviews:\n",
    "    gt_interviews_list[company] = [company for company in gt_interviews[company]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 Create SBI ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbi = pd.read_csv('../remaining_info/companies_SBI_overview.csv', sep=\";\")[0:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sbicodes(companyName, df):\n",
    "    df = df.loc[df['DisplayName'] == companyName]\n",
    "    sbi =  df[['SBI_1', 'SBI_2', 'SBI_3', 'SBI_4', 'SBI_5', 'SBI_6', 'SBI_7', 'SBI_8']]\n",
    "    if len(sbi) == 1 : sbi_values =  sbi.values.tolist()[0]\n",
    "    else : print('somehow we had multiple lines with only one company name.')\n",
    "    return [value for value in sbi_values if math.isnan(value) == False]\n",
    "\n",
    "\n",
    "company_sbi_dict = {}\n",
    "\n",
    "for company in company_names:    \n",
    "    company_sbi_dict[company] = extract_sbicodes(company, df_sbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sbicodes = {}\n",
    "\n",
    "for main_company in company_names:\n",
    "    similar_sbi = []\n",
    "\n",
    "    current_company_sbi = company_sbi_dict[main_company]\n",
    "    other_companies = [value for value in company_sbi_dict.keys() if value != main_company]\n",
    "\n",
    "    for company in other_companies:\n",
    "        for SBI in company_sbi_dict[company]:\n",
    "            if SBI in current_company_sbi:\n",
    "                similar_sbi.append(company)\n",
    "            \n",
    "    gt_sbicodes[main_company] = [value for value in set(similar_sbi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Load results of one specific vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty):\n",
    "    '''Based on the values provided it extracts the given distance matrix from the results folder. It transforms this pandas\n",
    "    matrix into a numpy array which is an inner list containing all distance scores between all pairs of companies (90x90)'''\n",
    "    if IDF_penalty == 'yes': idf = '-TFIDF'\n",
    "    else: idf = ''\n",
    "\n",
    "    df_simalarityScores = pd.read_pickle('../data/results/SS_{}_{}_P{}_U{}{}.pkl'.format(data_input, distance_algorithm, post_level, user_level, idf))\n",
    "    company_names = [value for value in df_simalarityScores.index]\n",
    "    return df_simalarityScores.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIER MAAK IN EEN INNER DICTIONARY. HIERDOOR KUN JE DE AFSTAND TUSSEN TWEE BEDRIJVEN VINDEN MET:\n",
    "# values_dict[COMPANY_1][COMPANY_2]\n",
    "def obtain_research_gt(similarityValues, company_names=company_names):\n",
    "    \"\"\"Takes the numpy array created with the 'load_distance_matrix' as input. It transforms this inner list to \n",
    "    an inner dict so we can simply obtain a distance metric between two companies.\"\"\"\n",
    "    own_method = {}\n",
    "    \n",
    "    lijst_index = 0\n",
    "    for lijst in similarityValues:\n",
    "        lijst_dict = {}\n",
    "        company_index = 0\n",
    "\n",
    "        for value in lijst:\n",
    "            lijst_dict[company_names[company_index]] = similarityValues[lijst_index][company_index]    \n",
    "            company_index += 1\n",
    "\n",
    "        own_method[company_names[lijst_index]] = lijst_dict\n",
    "        lijst_index += 1\n",
    "    return own_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Evaluate results with either SBI or Interview gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Calculate precision, recall for all companies combined with threshold 0.01 until 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ownmethod_competitors(company, threshold, method_results, similarityMetric):\n",
    "    if similarityMetric in ['cosine', 'jaccard']:\n",
    "        above_threshold = [key for key in method_results if method_results[company][key] > threshold]\n",
    "    elif similarityMetric in ['euclidean']:\n",
    "        above_threshold = [key for key in method_results if method_results[company][key] < threshold]\n",
    "    return [value for value in above_threshold if value != company]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(own_results, ground_truth, distance_metric):\n",
    "    '''The own_results are the companies identified as competitors by my own created tool. These are obtained\n",
    "    with the ownmethod_competitors function. By comparing this with a ground truth we can obtain the metrics.'''\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    #We did not identify any companies as competitors\n",
    "    if len(own_results) == 0:\n",
    "        return {'precision' : 1.0, 'recall' : 0.0, 'TP' : 0, 'FP': 0, 'FN' : 0}\n",
    "    else:\n",
    "        for company in own_results:\n",
    "            \n",
    "            if company in ground_truth:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        for company in ground_truth:\n",
    "            if company not in own_results:\n",
    "                FN += 1\n",
    "    precision = TP / (TP + FP)\n",
    "    if FN == 0:\n",
    "        recall = 1\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "    \n",
    "    return {'precision' : precision, 'recall' : recall, 'TP' : TP, 'FP' : FP, 'FN' : FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1_score(recall,precision):\n",
    "    \"\"\"The f1 score is the Harmonic meanof Precision and Recall.\"\"\" \n",
    "    if recall == 0:\n",
    "        return 0\n",
    "    if precision == 0:\n",
    "        return 0\n",
    "    return 2 / ((1/recall) + (1/precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(own_results, ground_truth, all_companies):\n",
    "    '''The own_results are the companies identified as competitors by my own created tool. These are obtained\n",
    "    with the ownmethod_competitors function. By comparing this with a ground truth we can obtain the metrics.'''\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "\n",
    "    for company in all_companies:\n",
    "        if company not in ground_truth:\n",
    "            if company not in own_results:\n",
    "                TN += 1\n",
    "            if company in own_results:\n",
    "                FP += 1\n",
    "        if company in ground_truth:\n",
    "            if company in own_results:\n",
    "                TP += 1\n",
    "            if company not in own_results:\n",
    "                FN += 1\n",
    "        \n",
    "    return {'TP' : TP, 'FP' : FP, 'FN' : FN, 'TN' : TN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(dict_values):\n",
    "    '''This function takes as input a dictionary with FP, TN, TP and FN values. It returns the recall,\n",
    "    precision and f1 score for that set of values.'''\n",
    "    try:\n",
    "        precision = dict_values['TP'] / (dict_values['TP'] + dict_values['FP'])\n",
    "    except ZeroDivisionError:\n",
    "        if dict_values['TP'] + dict_values['FP'] == 0:\n",
    "            precision = 1.0\n",
    "        else:\n",
    "            precision = 0.0\n",
    "    \n",
    "    try:\n",
    "        recall = dict_values['TP'] / (dict_values['TP'] + dict_values['FN'])\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "    \n",
    "    try:\n",
    "        f1_score = calc_f1_score(recall, precision)\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 0.0\n",
    "        \n",
    "    return {'precision' : precision, 'recall' : recall, 'f1_score' : f1_score}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def overall_metrics_all_companies(company_names, threshold, gt_research, ground_truth, similarityMetric, debugging):\n",
    "    \"\"\"Give this function one specific threshold and it calculates the average precision and recall for all companies\n",
    "    combined. This function is used to find the best threshold for all companies in general.\"\"\"\n",
    "    confusion_values_overall = {'TP':0, 'FP':0, 'FN':0, 'TN':0}\n",
    "    # Define the set of all companies\n",
    "    #if len([company for company in ground_truth]) == 9:\n",
    "    #    company_names = [company for company in ground_truth]\n",
    "    \n",
    "    for company in company_names:\n",
    "        #Define what the competitors of this business are at the given threshold\n",
    "        own_results = ownmethod_competitors(company, threshold, gt_research, similarityMetric)\n",
    "        groundtruth = [value for value in ground_truth[company]]\n",
    "        \n",
    "        if debugging != None:\n",
    "            print(threshold)\n",
    "            print(own_results)\n",
    "            print('')\n",
    "            print(groundtruth)\n",
    "            print('')\n",
    "        \n",
    "        #Create the confusion matrix compared to the ground_truth is for this threshold\n",
    "        confusion_values = confusion_matrix(own_results, groundtruth, [value for value in gt_research])\n",
    "        #Add them all up so we can create a confusion matrix for all companies\n",
    "        confusion_values_overall['TP'] += confusion_values['TP']\n",
    "        confusion_values_overall['FP'] += confusion_values['FP']\n",
    "        confusion_values_overall['FN'] += confusion_values['FN']\n",
    "        confusion_values_overall['TN'] += confusion_values['TN']\n",
    "    \n",
    "        if debugging != None:\n",
    "            print(confusion_values)\n",
    "            print('')\n",
    "            print('')\n",
    "            print('')\n",
    "    return calculate_metrics(confusion_values_overall)\n",
    "\n",
    "        \n",
    "\n",
    "def performance_thresholds_all_companies(company_names, gt_research, ground_truth, similarityMetric, debugging):\n",
    "    \"\"\"Here we basically use the ovarall_metrics_all_companies function ranging with a threshold from 0.01 until 1\n",
    "    with steps of 0,01. It calculates the metrics for all these thresholds.\"\"\"\n",
    "    overall_statistics = []\n",
    "    for i in range(1,100):\n",
    "        threshold = i/100\n",
    "    \n",
    "        metrics = overall_metrics_all_companies(company_names, threshold, gt_research, ground_truth, similarityMetric, debugging)    \n",
    "        overall_statistics.append([threshold,\n",
    "                                  metrics['recall'],\n",
    "                                  metrics['precision'],\n",
    "                                  metrics['f1_score']])\n",
    "                              \n",
    "    return pd.DataFrame(overall_statistics, columns=['threshold', 'recall', 'precision', 'f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_threshold(model_stats):\n",
    "    \"\"\"This function uses the input of the performance threshholds function to give as result which \n",
    "    threshold for the given company resulted in the best f1 score.\"\"\"\n",
    "    index_best_f1score = model_stats['f1_score'].argmax()\n",
    "    return dict(model_stats.iloc[index_best_f1score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all code above to find best threshold & performance of given similarity matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas keeps giving warnings because a feature is going to be replaced soon, this codes ignores the warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.043,\n",
       " 'recall': 0.4189189189189189,\n",
       " 'precision': 0.2767857142857143,\n",
       " 'f1_score': 0.3333333333333333}"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining all code above\n",
    "\n",
    "data_input = 'hashtag'\n",
    "distance_algorithm = 'cosine'\n",
    "post_level = 'relative'\n",
    "user_level = 'absolute'\n",
    "IDF_penalty = 'yes'\n",
    "if IDF_penalty == 'yes': idf = '-TFIDF'\n",
    "else: idf = ''\n",
    "\n",
    "#### CHANGE THE GT_INTERVIEWS TO gt_sbicodes TO EVALUATE DISTANCE MATRIX WITH SBI GOLD STANDARD.\n",
    "gt_research = obtain_research_gt(load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty))\n",
    "all_comps = performance_thresholds_all_companies(company_names, gt_research, gt_interviews, distance_algorithm, None)\n",
    "choose_best_threshold(all_comps)\n",
    "gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to apply K-fold cross validation. Model is trained with 70% of data, evaluation scores obtained by testing model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def crossValidationSBI(data_input, distance_algorithm, post_level, user_level, idf, groundtruth = gt_sbicodes):\n",
    "    if IDF_penalty == 'yes': idf = '-TFIDF'\n",
    "    else: idf = ''\n",
    "    cross_validation_performances = {}\n",
    "        \n",
    "    gt_research = obtain_research_gt(load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty))\n",
    "    \n",
    "    for i in range(100):\n",
    "        company_names = [value for value in groundtruth]\n",
    "        train_portion = 0.7\n",
    "        trainset = random.sample(company_names, int(len(company_names) * train_portion))\n",
    "        testset = [value for value in company_names if value not in trainset]\n",
    "\n",
    "        #Here we create a model which is overfitting the traindataset\n",
    "        performance_trainset = performance_thresholds_all_companies(trainset, gt_research, groundtruth, distance_algorithm, None)\n",
    "        metrics_trainset = choose_best_threshold(performance_trainset)\n",
    "        # This was the threshold best at overfitting the train dataset\n",
    "        selected_threshold = metrics_trainset['threshold']\n",
    "\n",
    "        ## Lets now analyse what the performance of this model is on the test dataset\n",
    "        final_performance = overall_metrics_all_companies(testset, selected_threshold, gt_research, groundtruth, distance_algorithm, None)\n",
    "        final_performance['threshold'] = selected_threshold\n",
    "        cross_validation_performances[i] = final_performance\n",
    "    \n",
    "    return pd.DataFrame(cross_validation_performances).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidationInterview(data_input, distance_algorithm, post_level, user_level, IDF_penalty, groundtruth = gt_interviews):\n",
    "    if IDF_penalty == 'yes': idf = '-TFIDF'\n",
    "    else: idf = ''\n",
    "    cross_validation_performances = {}\n",
    "    \n",
    "    #Distance metrics calcualted by my study    \n",
    "    gt_research = obtain_research_gt(load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty))\n",
    "    \n",
    "    company_names = [value for value in groundtruth]\n",
    "    \n",
    "    for i in range(len(company_names)):\n",
    "        trainset = [comp for comp in company_names if comp != company_names[i]]\n",
    "        testset = [value for value in company_names if value not in trainset]\n",
    "\n",
    "        performance_trainset = performance_thresholds_all_companies(trainset, gt_research, groundtruth, distance_algorithm, None)\n",
    "        metrics_trainset = choose_best_threshold(performance_trainset)\n",
    "    \n",
    "        selected_threshold = metrics_trainset['threshold']\n",
    "        final_performance = overall_metrics_all_companies(testset, selected_threshold, gt_research, groundtruth, distance_algorithm, None)\n",
    "        final_performance['threshold'] = selected_threshold\n",
    "        cross_validation_performances[i] = final_performance\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(cross_validation_performances).transpose().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.263541</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.741892</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.028016</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.284489</td>\n",
       "      <td>0.038586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.196787</td>\n",
       "      <td>0.126404</td>\n",
       "      <td>0.271868</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.243631</td>\n",
       "      <td>0.154042</td>\n",
       "      <td>0.409685</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.264016</td>\n",
       "      <td>0.165861</td>\n",
       "      <td>0.819251</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.284337</td>\n",
       "      <td>0.179917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.330318</td>\n",
       "      <td>0.241433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1_score   precision      recall   threshold\n",
       "count  100.000000  100.000000  100.000000  100.000000\n",
       "mean     0.263541    0.168675    0.741892    0.059800\n",
       "std      0.028016    0.022556    0.284489    0.038586\n",
       "min      0.196787    0.126404    0.271868    0.020000\n",
       "25%      0.243631    0.154042    0.409685    0.020000\n",
       "50%      0.264016    0.165861    0.819251    0.030000\n",
       "75%      0.284337    0.179917    1.000000    0.100000\n",
       "max      0.330318    0.241433    1.000000    0.100000"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossValidationSBI('hashtag', 'euclidean', 'relative', 'absolute', 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Createe Baseline models (SBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.273367</td>\n",
       "      <td>0.159553</td>\n",
       "      <td>0.968327</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029505</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.037792</td>\n",
       "      <td>0.035485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.255519</td>\n",
       "      <td>0.147002</td>\n",
       "      <td>0.960277</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.273333</td>\n",
       "      <td>0.158920</td>\n",
       "      <td>0.981503</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.292320</td>\n",
       "      <td>0.172123</td>\n",
       "      <td>0.990178</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.365288</td>\n",
       "      <td>0.225124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1_score    precision       recall    threshold\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
       "mean      0.273367     0.159553     0.968327     0.031300\n",
       "std       0.029505     0.020036     0.037792     0.035485\n",
       "min       0.176991     0.097087     0.648276     0.010000\n",
       "25%       0.255519     0.147002     0.960277     0.010000\n",
       "50%       0.273333     0.158920     0.981503     0.020000\n",
       "75%       0.292320     0.172123     0.990178     0.040000\n",
       "max       0.365288     0.225124     1.000000     0.320000"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * Ran evaluation of randomized model 100 times. Each run was done with 10-fold cross validation.times. \n",
    "\n",
    "%%time\n",
    "baseline_model = pd.DataFrame()\n",
    "\n",
    "for i in range(100):\n",
    "    run_df = crossValidationSBI()\n",
    "    baseline_model = pd.concat([baseline_model, run_df], ignore_index=True, sort=True)\n",
    "    \n",
    "\n",
    "SBI_baselineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Visualize how the recall and precision varies, show best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df_metrics(dataframe_metrics):\n",
    "    model_stats = dataframe_metrics.to_dict()\n",
    "\n",
    "\n",
    "    x1 = [model_stats['threshold'][value] for value in model_stats['threshold']]\n",
    "    y1 = [model_stats['precision'][value] for value in model_stats['precision']]\n",
    "\n",
    "    # plotting the line 1 points \n",
    "    plt.plot(x1, y1, label = \"precision\")\n",
    "    # line 2 points\n",
    "    x2 = [model_stats['threshold'][value] for value in model_stats['threshold']]\n",
    "    y2 = [model_stats['recall'][value] for value in model_stats['recall']]\n",
    "    # plotting the line 2 points \n",
    "    plt.plot(x2, y2, label = \"recall\")\n",
    "\n",
    "    plt.xlabel('threshold')\n",
    "    # Set the y axis label of the current axis.\n",
    "    plt.ylabel('precision/recall')\n",
    "    # Set a title of the current axes.\n",
    "    plt.title('Different thresholds when companies identified as competitors')\n",
    "    # show a legend on the plot\n",
    "    plt.legend()\n",
    "    # Display a figure.\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_metrics(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_best_threshold(all_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Compare gt_interview with gt_SBI codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.17763157894736842,\n",
       " 'recall': 0.36486486486486486,\n",
       " 'f1_score': 0.23893805309734514,\n",
       " 'TP': 27,\n",
       " 'FP': 125,\n",
       " 'FN': 47}"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_ground_truths(gt_interviews, gt_sbicodes):\n",
    "    \"\"\"Calculates the metrics of SBIcodes with respect to interviews\"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for company in gt_interviews:\n",
    "        ground_truth = gt_interviews[company]\n",
    "        own_results = gt_sbicodes[company]\n",
    "\n",
    "        for comp in own_results:\n",
    "            if comp in ground_truth:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        for comp in ground_truth:\n",
    "            if comp not in own_results:\n",
    "                FN += 1\n",
    "        \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = calc_f1_score(recall,precision)\n",
    "    \n",
    "    return {'precision' : precision, 'recall' : recall, 'f1_score' : f1_score, 'TP' : TP, 'FP' : FP, 'FN' : FN}\n",
    "\n",
    "compare_ground_truths(gt_interviews_list, gt_sbicodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Calculate Sector distinguishing metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_difference(companyName, selection, gt_research):\n",
    "    company_distance = []\n",
    "    for company in selection:\n",
    "        company_distance.append(gt_research[companyName][company])\n",
    "    return sum(company_distance) / len(company_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "import numpy as np\n",
    "\n",
    "def compute_sectorDis(data_input, distance_algorithm, post_level, user_level, IDF_penalty, company_names=company_names):\n",
    "    if IDF_penalty == 'yes': idf = '-TFIDF'\n",
    "    else: idf = ''\n",
    "    \n",
    "    gt_research = obtain_research_gt(load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty))\n",
    "\n",
    "    company_industry_matrix = {}\n",
    "    companyTypes = [furniture_companies, sport_companies, food_companies]\n",
    "    TypeNames = ['furniture', 'sport', 'food']\n",
    "    \n",
    "    #### STEP 1 - CREATING THE DISTANCE MATRIX ####\n",
    "    for i in range(3): \n",
    "        companyType = companyTypes[i]\n",
    "        #Looping through all the companies in one of the three categories\n",
    "        category_averages = {}\n",
    "\n",
    "        furniture_values = [industry_difference(company, furniture_companies, gt_research) for company in companyType]\n",
    "        sport_values = [industry_difference(company, sport_companies, gt_research) for company in companyType]\n",
    "        food_values = [industry_difference(company, food_companies, gt_research) for company in companyType]\n",
    "\n",
    "\n",
    "        furniture_average = sum(furniture_values) / len(furniture_values)\n",
    "        sport_average = sum(sport_values) / len(sport_values)\n",
    "        food_average = sum(food_values) / len(food_values)\n",
    "\n",
    "        company_industry_matrix[TypeNames[i]] = {'furniture' : furniture_average,\n",
    "                                                'sport' : sport_average,\n",
    "                                                'food' : food_average}\n",
    "\n",
    "    average_matrix_industries = pd.DataFrame(company_industry_matrix)\n",
    "    # reversing order of columns so it reads easier as a matrix.\n",
    "    average_matrix_industries = average_matrix_industries[['food', 'furniture', 'sport']]\n",
    "    dataframe_dict = average_matrix_industries.to_dict()\n",
    "    print(dataframe_dict)\n",
    "    #### STEP 2 -- COMPUTING THE SECTOR DIFFERENCE VALUES\n",
    "    average_values = []\n",
    "\n",
    "    for industry in dataframe_dict:\n",
    "        inter_value = dataframe_dict[industry][industry]\n",
    "\n",
    "        for other_industry in dataframe_dict[industry]:\n",
    "            if other_industry != industry:\n",
    "                other_value = dataframe_dict[industry][other_industry]\n",
    "                #print(industry, other_industry)\n",
    "                #print( (inter_value - other_value) / other_value * 100 )\n",
    "                average_values.append( (inter_value - other_value) / other_value * 100 )\n",
    "                #print(' ')\n",
    "                \n",
    "    #### STEP 3 -- COMPUTING THE AVERAGE\n",
    "    return sum(average_values) / len(average_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food': {'food': 0.8607984195265673, 'furniture': 0.7594708843163696, 'sport': 0.7251137598281161}, 'furniture': {'food': 0.7594708843163696, 'furniture': 0.7772341752145724, 'sport': 0.7118629791726878}, 'sport': {'food': 0.7251137598281162, 'furniture': 0.7118629791726877, 'sport': 0.735548833728394}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.05707782148867"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sectorDis('text', 'cosine', 'absolute', 'absolute', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "import numpy as np\n",
    "def industry_difference(companyName, selection, gt_research):\n",
    "    company_distance = []\n",
    "    for company in selection:\n",
    "        company_distance.append(gt_research[companyName][company])\n",
    "    return sum(company_distance) / len(company_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>furniture</th>\n",
       "      <th>sport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.083465</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furniture</th>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.025541</td>\n",
       "      <td>0.003103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.081391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               food  furniture     sport\n",
       "food       0.083465   0.003718  0.002414\n",
       "furniture  0.003718   0.025541  0.003103\n",
       "sport      0.002414   0.003103  0.081391"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input = 'text'\n",
    "distance_algorithm = 'cosine'\n",
    "post_level = 'relative'\n",
    "user_level = 'relative'\n",
    "IDF_penalty = 'yes'\n",
    "\n",
    "gt_research = obtain_research_gt(load_distance_matrix(data_input, distance_algorithm, post_level, user_level, IDF_penalty))\n",
    "\n",
    "company_industry_matrix = {}\n",
    "companyTypes = [furniture_companies, sport_companies, food_companies]\n",
    "TypeNames = ['furniture', 'sport', 'food']\n",
    "\n",
    "for i in range(3): \n",
    "    companyType = companyTypes[i]\n",
    "    #Looping through all the companies in one of the three categories\n",
    "    category_averages = {}\n",
    "    \n",
    "    furniture_values = [industry_difference(company, furniture_companies, gt_research) for company in companyType]\n",
    "    sport_values = [industry_difference(company, sport_companies, gt_research) for company in companyType]\n",
    "    food_values = [industry_difference(company, food_companies, gt_research) for company in companyType]\n",
    "    \n",
    "    \n",
    "    furniture_average = sum(furniture_values) / len(furniture_values)\n",
    "    sport_average = sum(sport_values) / len(sport_values)\n",
    "    food_average = sum(food_values) / len(food_values)\n",
    "    \n",
    "    company_industry_matrix[TypeNames[i]] = {'furniture' : furniture_average,\n",
    "                                            'sport' : sport_average,\n",
    "                                            'food' : food_average}\n",
    "\n",
    "average_matrix_industries = pd.DataFrame(company_industry_matrix)\n",
    "# reversing order of columns so it reads easier as a matrix.\n",
    "average_matrix_industries = average_matrix_industries[['food', 'furniture', 'sport']]\n",
    "average_matrix_industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe_dict = average_matrix_industries.to_dict()\n",
    "average_values = []\n",
    "\n",
    "for industry in dataframe_dict:\n",
    "    inter_value = dataframe_dict[industry][industry]\n",
    "    \n",
    "    for other_industry in dataframe_dict[industry]:\n",
    "        if other_industry != industry:\n",
    "            other_value = dataframe_dict[industry][other_industry]\n",
    "            #print(industry, other_industry)\n",
    "            #print( (inter_value - other_value) / other_value * 100 )\n",
    "            average_values.append( (inter_value - other_value) / other_value * 100 )\n",
    "            #print(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
